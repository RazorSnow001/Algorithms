Multithreaded Algorithms

The vast majority of algorithms in this book are serial algorithms suitable for
running on a uni processor computer in which only one instruction executes at a
time.

In this chapter, we shall extend our algorithmic model to encompass parallel
algorithms, which can run on a multiprocessor computer that permits multiple
instructions to execute concurrently

dynamic multithreaded algorithms

The first lecture teaches the basics behind
multithreading, including defining the measures of work and critical-path length

critical path length !
what is that ?

greedy scheduling theorem

The second lecture shows
how parallel applications, including matrix multiplication and sorting, can be analyzed using
divide-and-conquer recurrences

matrix multiplication

sort !

Multithreaded programming is a programming paradigm in which a single program
is broken into multiple threads of control which interact to solve a single problem

where threads can
be created and destroyed as easily as an ordinary subroutine can be called and return.

The keyword spawn before the
subroutine call in line 3 indicates that the subprocedure FIB(n − 1) can execute in parallel with
the procedure FIB(n) itself

that is very great !
that is very great !

Unlike an ordinary function call, however, where the parent is not
resumed until after its child returns  in the case of a spawn, the parent can continue to execute in
parallel with the child

In this case, the parent goes on to spawn FIB(n − 2). In general, the parent
can continue to spawn off children, producing a high degree of parallelism.

so
the spawn of the program means we can
the parent nodes needn't  to wait its child nodes return .
but eventually we need the result !
but as the very early stage . we can let the bullet fly for a while
!
don't wait it and just move on to the next line
that is quick
asyn style !
quite interesting my bro !

A procedure cannot safely use the return values of the children it has spawned until it executes
a sync statement.

If any of its children have not completed when it executes a sync, the procedure
suspends and does not resume until all of its children have completed

but the syn is as important as the asyn
without the syn we can't get the desired result !

so we have to wait for the corresponding sub problem finish their task !
waiting for the sub problem finish ! !

-------------------------------------         ------
         -------------------------------------
         -------------------------------------

----------------------------------------- ---------------------------------------- ---------------------------------------- ------

the overall execution time is less !

but we also need to consider the cost of creating the new thread !  and even close  them .

so the main the parent node touch the sync statement and check whether the sub node have return ?
yes both
we return the results !


In the Fibonacci example, the sync statement in line 5 is required before the return statement
in line 6 to avoid the anomaly that would occur if x and y were summed before each had been
computed

The spawn and sync keywords specify logical parallelism, not “actual” parallelism

these keywords indicate which code may possibly execute in parallel, but what actually runs in
parallel is determined by a scheduler

which maps the dynamically unfolding computation onto
the available processors.

e can view a multithreaded computation in graph-theoretic terms as a dynamically unfolding
dag G = (V, E)

We define a thread to be a maximal sequence
of instructions not containing the parallel control statements spawn , sync, and return

Threads
make up the set V of vertices of the multithreaded computation dag G.

Each procedure execution is
a linear chain of threads, each of which is connected to its successor in the chain by a continuation
edge.


the critical path of the graph is 8 we find that is the longest path of the the execution .

so we need to learn how to analyze the master theory to handle the recursion